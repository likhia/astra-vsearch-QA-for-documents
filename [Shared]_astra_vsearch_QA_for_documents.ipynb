{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likhia/astra-vsearch-QA-for-documents/blob/main/%5BShared%5D_astra_vsearch_QA_for_documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf05e3f2"
      },
      "source": [
        "# astra-vsearch-QA-for-documents\n",
        "This demo guides you through setting up Astra DB with Vector Search, Cassio and Open AI to implement an generative Q&A for your own Documentation\n",
        "\n",
        "Jupyter notebook for generative Q&A for douments is powered by [Astra Vector Search](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html) and [OpenAI](https://github.com/openai/) and Casssio [Opensource LLM integration with Cassandra and Astra DB](https://cassio.org/).\n",
        "\n",
        "## Astra Vector Search\n",
        "Astra vector search enables developers to search a database by context or meaning rather than keywords or literal values. This is done by using “embeddings”. Embeddings are a type of representation used in machine learning where high-dimensional or complex data is mapped onto vectors in a lower-dimensional space. These vectors capture the semantic properties of the input data, meaning that similar data points have similar embeddings.\n",
        "Reference: [Astra Vector Search](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html)\n",
        "\n",
        "## CassIO\n",
        "CassIO is the ultimate solution for seamlessly integrating Apache Cassandra® with generative artificial intelligence and other machine learning workloads. This powerful Python library simplifies the complicated process of accessing the advanced features of the Cassandra database, including vector search capabilities. With CassIO, developers can fully concentrate on designing and perfecting their AI systems without any concerns regarding the complexities of integration with Cassandra.\n",
        "Reference [Cassio](https://cassio.org/)\n",
        "\n",
        "## OpenAI\n",
        "OpenAI provides various tools and resources to implement your own Document QA Search system. This includes pre-trained language models like GPT-3.5, which can understand and generate human-like text. Additionally, OpenAI offers guidelines and APIs to leverage their models for document search and question-answering tasks, enabling developers to build powerful and intelligent Document QA Search applications.\n",
        "Reference: [OpenAI](https://github.com/openai/)\n",
        "\n",
        "## Demo Summary\n",
        "ChatGPT excels at answering questions, but only on topics it remembers from its training data. It offers you a nice dialog interface to ask questions and get answers.\n",
        "\n",
        "But what do you do when you have your onw documents? How can you leverage the GenAI and LLM models to get insights in those?\n",
        "\n",
        "Think of an Q/A Bot that you want to provide to your customers for asking questions against the documentation of your products.\n",
        "\n",
        "For beeing able to do so, you have to implement your own ChatGPT-like solution.\n",
        "The implementation requires\n",
        "1. Analysing your existing documents and store the information\n",
        "2. Providing search capabilities for your questions to get answers\n",
        "\n",
        "This is solve by using a LLM models. Ideally you embedd the data as vectors and store them in a vector database and then use the LLM models on top of that database.\n",
        "\n",
        "This notebook demonstrates a two-step Search-Ask method for enabling GPT to answer questions using a library of reference on your onw documentations based on Astra DB vector search.\n",
        "\n",
        "\n"
      ],
      "id": "cf05e3f2"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zf94JViw27U5"
      },
      "id": "Zf94JViw27U5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "651400d1"
      },
      "source": [
        "# Getting Started with this notebook\n",
        "\n",
        "These are prerequisites you need to to before running this notebook\n",
        "- Create a new vector search enabled database in Astra.\n",
        "- Create a keyspace\n",
        "- Create a token with permissions to create tables\n",
        "- Download your secure-connect-bundle.zip file\n",
        "- Create an OpenAI account and download an API Key\n",
        "\n",
        "- When you run this notebook, it will ask you for providing the secure-connect-bundle.zip, some text file and client ids, passwords as well as API Key\n"
      ],
      "id": "651400d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "311cc2e9"
      },
      "source": [
        "# Setup\n",
        "\n",
        "This jupyter notebook was build on Colab. You need to install the following libraries."
      ],
      "id": "311cc2e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6d88d66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6513f6ce-1c63-4223-afa2-bb6d6535ec81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cassandra-driver>=3.28.0\n",
            "  Downloading cassandra_driver-3.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.27.7\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken==0.4.0\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain>=0.0.218\n",
            "  Downloading langchain-0.0.237-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cassio==0.0.7\n",
            "  Downloading cassio-0.0.7-py3-none-any.whl (10 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.12.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.0/255.0 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.7) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.7) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.7) (3.8.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from cassio==0.0.7) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver>=3.28.0) (1.16.0)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver>=3.28.0)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218) (2.0.18)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain>=0.0.218)\n",
            "  Downloading dataclasses_json-0.5.12-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.0.11,>=0.0.10 (from langchain>=0.0.218)\n",
            "  Downloading langsmith-0.0.10-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain>=0.0.218)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218) (1.10.11)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.7) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.7) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.7) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.7) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.7) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.7) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.218)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.218)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver>=3.28.0) (8.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain>=0.0.218) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.7) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.7) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.7) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.218) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.218) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.218)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pypdf, mypy-extensions, marshmallow, geomet, typing-inspect, tiktoken, openapi-schema-pydantic, langsmith, cassandra-driver, openai, dataclasses-json, cassio, langchain\n",
            "Successfully installed cassandra-driver-3.28.0 cassio-0.0.7 dataclasses-json-0.5.12 geomet-0.2.1.post1 langchain-0.0.237 langsmith-0.0.10 marshmallow-3.19.0 mypy-extensions-1.0.0 openai-0.27.7 openapi-schema-pydantic-1.2.4 pypdf-3.12.2 tiktoken-0.4.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# install required dependencies\n",
        "! pip install \\\n",
        "    \"cassandra-driver>=3.28.0\" \\\n",
        "    \"openai==0.27.7\" \\\n",
        "    \"tiktoken==0.4.0\" \\\n",
        "    \"langchain>=0.0.218\" \\\n",
        "    \"cassio==0.0.7\" \\\n",
        "    \"pypdf\""
      ],
      "id": "a6d88d66"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae20e0b5"
      },
      "source": [
        "# Imports"
      ],
      "id": "ae20e0b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16f9f33a"
      },
      "outputs": [],
      "source": [
        "# Imports for our environment and accessing Astra DB\n",
        "import os\n",
        "\n",
        "import getpass\n",
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "from google.colab import files\n",
        "import openai"
      ],
      "id": "16f9f33a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4df2888e"
      },
      "source": [
        "# Astra DB configuration, connection bundle and token secrets\n",
        "\n",
        "You will need a secure connect bundle and a user with access permission. For demo purposes the \"administrator\" role will work fine.\n",
        "More information about how to get the bundle can be found [here](https://docs.datastax.com/en/astra-serverless/docs/connect/secure-connect-bundle.html)."
      ],
      "id": "4df2888e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvY0HTqm3chY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "6514d2bf-0a70-4e14-b48f-0ab2d40d8ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Secure Connect Bundle\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5be5d111-b395-4926-9529-ccf5c84d18de\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5be5d111-b395-4926-9529-ccf5c84d18de\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving secure-connect-vector2.zip to secure-connect-vector2.zip\n"
          ]
        }
      ],
      "source": [
        "#upload secure connect bundle\n",
        "print('Please upload your Secure Connect Bundle')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
        "    SECURE_CONNECT_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
        "else:\n",
        "    raise ValueError(\n",
        "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
        "    )\n",
        "#Alternatively upload to the environment and reference it here\n",
        "#SECURE_CONNECT_BUNDLE_PATH = '/content/secure-connect-documentation.zip'\n"
      ],
      "id": "QvY0HTqm3chY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39b4b87b"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_TOKEN_BASED_USERNAME = getpass.getpass('What Astra DB token username do you want to use? ')\n",
        "#ASTRA_DB_TOKEN_BASED_USERNAME = '<<ENTER>>'\n",
        "\n"
      ],
      "id": "39b4b87b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38EAz0Kupms-"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_TOKEN_BASED_PASSWORD = getpass.getpass('What Astra DB token password do you want to use? ')\n",
        "#ASTRA_DB_TOKEN_BASED_PASSWORD = '<<ENTER>>'\n"
      ],
      "id": "38EAz0Kupms-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZqO7R66hmMU"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_KEYSPACE = input(f'Which Astra DB keypsace do you want to use? ')\n",
        "#ASTRA_DB_KEYSPACE = 'mykeyspace'\n"
      ],
      "id": "YZqO7R66hmMU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyZ1IC4d3U15"
      },
      "source": [
        "# Provide Sample Data\n",
        "If you want to provide some docoments, you can upload them here.\n",
        "As a sample document you can also download some text here:"
      ],
      "id": "tyZ1IC4d3U15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VsVGojG663U",
        "outputId": "1cb7b195-2565-4780-e9ee-c59b6101a0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 13022  100 13022    0     0   129k      0 --:--:-- --:--:-- --:--:--  129k\n"
          ]
        }
      ],
      "source": [
        "# Please skip this if you are not testing text file.\n",
        "# retrieve the text of a short story that will be indexed in the vector store\n",
        "! curl https://raw.githubusercontent.com/CassioML/cassio-website/main/docs/frameworks/langchain/texts/amontillado.txt --output amontillado.txt\n",
        "SAMPLEDATA = [\"amontillado.txt\"]"
      ],
      "id": "1VsVGojG663U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjxwchCO3n_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d4176f37-a0fe-4192-a42a-569fea71d8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your own sample file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ccd128b-3856-4b30-9b9f-06fdcfaecdd9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0ccd128b-3856-4b30-9b9f-06fdcfaecdd9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Whitepaper_AstraDB-Designing-Serverless-Cloud-Native-DBaaS_6141_07.22.21.pdf to Whitepaper_AstraDB-Designing-Serverless-Cloud-Native-DBaaS_6141_07.22.21.pdf\n",
            "File Uploaded\n"
          ]
        }
      ],
      "source": [
        "# Alternatively you can provide your own file - please consider to customize the queries at the end of the notebook to match your content.\n",
        "\n",
        "#only use if you want to test your onw files\n",
        "\n",
        "#provide some sample files\n",
        "print('Please upload your own sample file:')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    #sampleDataFileTitle = list(uploaded.keys()[0])\n",
        "    #SAMPLEDATA_PATH = os.path.join(os.getcwd(), sampleDataFileTitle)\n",
        "    SAMPLEDATA = uploaded\n",
        "else:\n",
        "    raise ValueError(\n",
        "        'Cannot proceed without Sample Data. Please re-run the cell.'\n",
        "    )\n",
        "\n",
        "print(f'File Uploaded')"
      ],
      "id": "PjxwchCO3n_8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33e090e7"
      },
      "source": [
        "# Connect to Astra DB"
      ],
      "id": "33e090e7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8c527a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8f65b3-0d8e-429a-b23d-94b41938b8f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 204bdb00-f5c8-420a-a68d-9c26142b0d38-us-east1.db.astra.datastax.com:29042:1a3e9df8-9ad9-46b2-9c4d-2df267a906a9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 204bdb00-f5c8-420a-a68d-9c26142b0d38-us-east1.db.astra.datastax.com:29042:1a3e9df8-9ad9-46b2-9c4d-2df267a906a9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(132580212726736) 204bdb00-f5c8-420a-a68d-9c26142b0d38-us-east1.db.astra.datastax.com:29042:1a3e9df8-9ad9-46b2-9c4d-2df267a906a9> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 204bdb00-f5c8-420a-a68d-9c26142b0d38-us-east1.db.astra.datastax.com:29042:1a3e9df8-9ad9-46b2-9c4d-2df267a906a9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ],
      "source": [
        "# make sure that you can connect to Astra DB - if you see errors, then have a look at the environment you configured earlier\n",
        "\n",
        "cloud_config = {\n",
        "   'secure_connect_bundle': SECURE_CONNECT_BUNDLE_PATH\n",
        "}\n",
        "auth_provider = PlainTextAuthProvider(ASTRA_DB_TOKEN_BASED_USERNAME, ASTRA_DB_TOKEN_BASED_PASSWORD)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()"
      ],
      "id": "e8c527a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8251382e"
      },
      "source": [
        "# Read files, Create Embeddings, Store in Vector DB\n",
        "\n",
        "CassIO seamlessly integrates with LangChain, offering Cassandra-specific tools for many tasks. In our example we will use vector stores, indexers, embeddings and queries.\n",
        "\n",
        "And we will use OpenAI for our LLM services. (See Pre-requisites on [cassio.org](https://cassio.org/start_here/#llm-access) for more details)."
      ],
      "id": "8251382e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5VPZclr6n67"
      },
      "outputs": [],
      "source": [
        "# Set your secret(s) for LLM access:\n",
        "# we will use OpenAI embeddings, so please provide your OpenAI AKP Key\n",
        "apiSecret = getpass.getpass('Your secret for LLM provider OpenAI: ')\n",
        "\n",
        "openai.api_key = apiSecret\n",
        "os.environ['OPENAI_API_KEY'] = apiSecret"
      ],
      "id": "J5VPZclr6n67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE5pZsfs7iPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee2bb2a-0831-426d-b4b3-e195569af744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed PDF file: Whitepaper_AstraDB-Designing-Serverless-Cloud-Native-DBaaS_6141_07.22.21.pdf\n",
            "\n",
            "Processing done.\n"
          ]
        }
      ],
      "source": [
        "#Import the needed libraries and declare the LLM model\n",
        "import langchain\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Cassandra\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# define the embedding function to use\n",
        "embedding_function = OpenAIEmbeddings()\n",
        "\n",
        "#define the table name to be used to store our embeddings, Cassio will create the objects in Astra DB for you.\n",
        "ASTRA_DB_TABLE_NAME = 'vdocuments'\n",
        "\n",
        "\n",
        "# now loop through all files uploaded and process them\n",
        "for elem in SAMPLEDATA:\n",
        "  filename = elem\n",
        "  doc_path = os.path.join(os.getcwd(), filename)\n",
        "\n",
        "  # check which filetype they are and parse them (loader)\n",
        "  # check if the file is a PDF\n",
        "  if filename.endswith(\".pdf\"):\n",
        "\t\t# load the PDF file\n",
        "    loader = PyPDFLoader(doc_path)\n",
        "    pages = loader.load_and_split()\n",
        "    print(f\"Processed PDF file: {filename}\")\n",
        "\n",
        "  # check if the file is a TXT\n",
        "  elif filename.endswith(\".txt\"):\n",
        "    loader = TextLoader(doc_path)\n",
        "    pages = loader.load_and_split()\n",
        "    print(f\"Processed TXT file: {filename}\")\n",
        "\n",
        "  # other files will not be processed\n",
        "  else:\n",
        "    # handle the case where the file has an unsupported extension\n",
        "    print(f\"Unsupported file type: {filename}\")\n",
        "\n",
        "  noOfPages = 0\n",
        "\n",
        "  # if any file was loaded, proceed\n",
        "  if len(pages) >0:\n",
        "\n",
        "    #create a vector store object for the document, that will automatically embedd it\n",
        "    cassVStore = Cassandra.from_documents(\n",
        "      documents=pages,\n",
        "      embedding=embedding_function,\n",
        "      session=session,\n",
        "      keyspace=ASTRA_DB_KEYSPACE,\n",
        "      table_name=ASTRA_DB_TABLE_NAME,\n",
        "    )\n",
        "\n",
        "    noOfPages = len(pages)\n",
        "\n",
        "    # now clean up\n",
        "    # delete the file\n",
        "    os.remove(doc_path)\n",
        "    # empty pages\n",
        "    pages = \"\"\n",
        "\n",
        "# empty the list of file names, just in case this block is run twice.\n",
        "SAMPLEDATA = []\n",
        "\n",
        "print(f\"\\nProcessing done.\")"
      ],
      "id": "TE5pZsfs7iPT"
    },
    {
      "cell_type": "code",
      "source": [
        "# just in case this demo runs multiple times and you want to clean up, run this:\n",
        "# cassVStore.clear()"
      ],
      "metadata": {
        "id": "U-GWCC2cCHgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1c9830d5-5fab-4174-c14f-251970a98f7b"
      },
      "id": "U-GWCC2cCHgF",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d835d3574502>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# just in case this demo runs multiple times and you want to clean up, run this:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcassVStore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cassVStore' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Query the Data and execute some \"searches\" against it\n",
        "First we will start with a similarity search using the Vectorstore's implementation"
      ],
      "metadata": {
        "id": "oTEglBjbaHDj"
      },
      "id": "oTEglBjbaHDj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Please change the prompt based on the PDF that is uploaded.\n",
        "# similarity search:\n",
        "# prompt = \"What is vector search?\"\n",
        "# prompt = \"what is embedding?\"\n",
        "prompt = \"What is Astra DB?\"\n",
        "\n",
        "# matched_docs is a list with the found documents from the similarity search\n",
        "matched_docs = cassVStore.similarity_search(prompt)\n",
        "# for each of the found documents, print the content\n",
        "for i, d in enumerate(matched_docs):\n",
        "    print(f\"\\n## Document {i}\\n\")\n",
        "    print(d.page_content)"
      ],
      "metadata": {
        "id": "yaFkjaAnCIRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cc59b4-2d79-4b2b-af49-ecdd2c0b5e90"
      },
      "id": "yaFkjaAnCIRw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Document 0\n",
            "\n",
            "WHITEPAPER\n",
            "D a t a S t a x  A s t r a  D B\n",
            "Designing a Serverless Cloud-Native\n",
            "Database-as-a-Service Based on Apache Cassandra™\n",
            "Astra DB is a globally distributed, serverless, multi-model\n",
            "database service built\n",
            "by DataStax to satisfy the needs of users on their\n",
            "cloud provider of choice. It is\n",
            "the ﬁrst and only serverless and multi-region database\n",
            "service that is based on an\n",
            "open-source NoSQL database, namely Apache Cassandra.\n",
            "In this paper, we share our experience, rationale\n",
            "and lessons learned of adapting\n",
            "Cassandra into a multi-tenant database to serve the\n",
            "serverless needs of Astra users.\n",
            "We present a novel microservices-based, cloud-native\n",
            "architecture that integrates\n",
            "natively with Kubernetes to bring true, safe, stateful\n",
            "workloads to the cloud-native age.\n",
            "This design enables ﬁne-grained, elastic scalability\n",
            "of individual components to meet\n",
            "the capacity demands of modern application workloads.\n",
            "In this work, our main\n",
            "contributions are: (i) novel microservices-based,\n",
            "cloud-native architecture; (ii)\n",
            "multi-tenancy and fault isolation approach; and (iii)\n",
            "smart auto scaling design.\n",
            "\n",
            "## Document 1\n",
            "\n",
            "0 7 \n",
            "C o n c l u s i o n s\n",
            "Astra DB launched Apache Cassandra™ into the space\n",
            "of cloud database services,\n",
            "serverless computing and microservices. To transform\n",
            "Cassandra into Astra DB, we\n",
            "designed a new microservices-based, cloud-native architecture,\n",
            "introduced a\n",
            "multi-tenancy and fault isolation approach, and implemented\n",
            "a smart auto scaling\n",
            "solution. As a result, DataStax Astra DB became the\n",
            "ﬁrst serverless and multi-cloud\n",
            "database built on Cassandra. Astra DB runs on AWS,\n",
            "GCP and Azure, safely supports\n",
            "thousands of tenants, and scales automatically with\n",
            "capacity demands. Ultimately, Astra\n",
            "DB is an easier and more cost-eﬃcient way to use\n",
            "Cassandra.\n",
            "© 2021 DataStax, All Rights Reserved. DataStax, Titan,\n",
            "and TitanDB are registered trademarks of\n",
            "DataStax, Inc. and its subsidiaries in the United\n",
            "States and/or other countries.\n",
            "Apache, Apache Cassandra, and Cassandra are either\n",
            "registered trademarks or trademarks of the\n",
            "Apache Software Foundation or its subsidiaries in\n",
            "Canada, the United States, and/or other countries.\n",
            "1 8\n",
            "Astra Serverless Whitepaper - July 2021\n",
            "\n",
            "## Document 2\n",
            "\n",
            "The high-level architecture of Astra DB is shown in\n",
            "Figure 3.1. Astra DB is composed of\n",
            "many smaller independent services that belong to the\n",
            "control plane, data plane,\n",
            "infrastructure services, and object storage services.\n",
            "Kubernetes\n",
            "is used to scale and\n",
            "orchestrate all services in the data plane, as well\n",
            "as infrastructure services.\n",
            "The control plane is responsible for maintaining and\n",
            "conﬁguring the data plane based on\n",
            "customer-speciﬁed settings and operational status\n",
            "reported by the data plane. The\n",
            "control plane services are beyond the scope of this\n",
            "paper.\n",
            "Data plane services are responsible for executing\n",
            "user and application data requests, as\n",
            "well as performing required database maintenance operations\n",
            "in the background. The\n",
            "data plane includes these types of services:\n",
            "Coordination Service\n",
            "is used for query coordination\n",
            "and data APIs. This service\n",
            "represents the open-source\n",
            "Stargate\n",
            "data gateway and\n",
            "supports REST, GraphQL,\n",
            "Document and CQL APIs to interact with the database.\n",
            "Data Service\n",
            "is responsible for reading, writing and\n",
            "managing a subset of data as\n",
            "deﬁned by its token range assignment based on the\n",
            "Cassandra ring. This service\n",
            "manages Cassandra’s in-memory data structures like\n",
            "Bloom ﬁlters and MemTables.\n",
            "It also relies upon fast, local NVMe volumes for caching\n",
            "data, commit log and\n",
            "partition index ﬁles. A local commit log is always\n",
            "synchronized with a commit log\n",
            "managed by the object storage services. Similarly,\n",
            "MemTables are ﬂushed to become\n",
            "SSTables managed by the object storage services.\n",
            "Compaction Service\n",
            "is utilized for compacting SSTables\n",
            "managed by the object\n",
            "storage services. Similarly to Cassandra’s compaction\n",
            "process, this service merges\n",
            "multiple SSTables into a smaller SSTable to optimize\n",
            "reads. Unlike Cassandra’s\n",
            "compaction process, this service is shared across\n",
            "tenants. It uses a uniﬁed\n",
            "compaction strategy and can also repair any inconsistent\n",
            "copies of data that may\n",
            "have been written by different data services.\n",
            "Commitlog Replayer Service\n",
            "is used for replaying a\n",
            "commit log managed by the\n",
            "object storage services in case when a data service\n",
            "fails. This service replays a\n",
            "commit log and writes a new SSTable managed by the\n",
            "object storage services.\n",
            "IAM Service\n",
            "is responsible for identity and access\n",
            "management in Astra DB. This\n",
            "service controls authentication and authorization\n",
            "based on predeﬁned roles and\n",
            "permissions deﬁned in the control plane.\n",
            "Controller Service for AIOps\n",
            "is designed to use analytics\n",
            "and machine learning to\n",
            "make intelligent decisions about operating Astra DB.\n",
            "This service is capable of\n",
            "analyzing both real-time and historical operational\n",
            "data to react to immediate issues\n",
            "and predict what may happen in the future. This is\n",
            "where decisions to scale a service\n",
            "up or down are made.\n",
            "7\n",
            "Astra Serverless Whitepaper - July 2021\n",
            "\n",
            "## Document 3\n",
            "\n",
            "Astra DB K8s Operator\n",
            "is created to help run and operate\n",
            "various Astra DB services\n",
            "using the Kubernetes operator paradigm. This deterministic\n",
            "service can start and\n",
            "stop other services, scale them up or down to maintain\n",
            "a proper state of the system.\n",
            "The infrastructure services include additional services\n",
            "that are based on existing\n",
            "open-source systems:\n",
            "Monitoring Service\n",
            "and\n",
            "Observability Service\n",
            "are responsible\n",
            "for collecting and\n",
            "visualizing metrics about Astra DB operations. These\n",
            "services are based on\n",
            "Prometheus and Grafana, respectively.\n",
            "Metadata Service\n",
            "is used for consistent management\n",
            "of database schema and\n",
            "cluster topology information. This service replaces\n",
            "Cassandra’s gossiping for peer\n",
            "discovery and state information propagation. It is\n",
            "based on the\n",
            "etcd\n",
            "key-value store.\n",
            "Last but not least,\n",
            "Object Storage Services\n",
            "rely on\n",
            "fully managed services to store data\n",
            "that Cassandra normally persists on disk, including\n",
            "commit logs, SSTables, indexes and\n",
            "other ﬁles. Depending on a cloud selection for deploying\n",
            "a database, this service can be\n",
            "represented by Amazon Simple Storage Service (S3),\n",
            "Google Cloud Storage (GCS), Azure\n",
            "Blob Storage (ABS), or any other cloud storage with\n",
            "an S3-compatible API.\n",
            "Figure 3.2\n",
            "Services Interaction in Astra DB\n",
            "8\n",
            "Astra Serverless Whitepaper - July 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Q/A Search using different ways\n"
      ],
      "metadata": {
        "id": "9cVulGW6aom5"
      },
      "id": "9cVulGW6aom5"
    },
    {
      "cell_type": "code",
      "source": [
        "# VectorStoreIndexWrapper allows for easy querying of existing data in a vector store\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "index = VectorStoreIndexWrapper(vectorstore=cassVStore)\n",
        "\n",
        "# Search within the document context for some text related information.\n",
        "index.query(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cptnBJhCWTz0",
        "outputId": "d58d517b-b3f9-4d21-e19e-a044384af4ea"
      },
      "id": "cptnBJhCWTz0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Astra DB is a globally distributed, serverless, multi-model database service built by DataStax to satisfy the needs of users on their cloud provider of choice. It is the first and only serverless and multi-region database service that is based on an open-source NoSQL database, namely Apache Cassandra.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://betterprogramming.pub/build-a-chatbot-on-your-csv-data-with-langchain-and-openai-ed121f85f0cd\n",
        "#Alternatively you can use a retrieval chain and some conversation history\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "#vectordb.persist()\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "pdf_qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.8) , cassVStore.as_retriever(), memory=memory)\n",
        "\n",
        "result = pdf_qa({\"question\": prompt})\n",
        "print(\"Answer :\", result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYz1Jdl1Qhuj",
        "outputId": "3e34a110-0bee-4563-9690-9495b74e7a8d"
      },
      "id": "kYz1Jdl1Qhuj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer :  Astra DB is a globally distributed, serverless, multi-model database service built by DataStax to satisfy the needs of users on their cloud provider of choice. It is the first and only serverless and multi-region database service that is based on an open-source NoSQL database, namely Apache Cassandra.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "message_objects = []\n",
        "\n",
        "# With the role as 'system',  we tell the model how we want it to behave and tell it how its personality and type of response should be.\n",
        "message_objects.append({\"role\":\"system\",\n",
        "                        \"content\":\"You're a chatbot to answer questions using the data provided.\"})\n",
        "\n",
        "# With the role as 'user',  pass the question from user.\n",
        "message_objects.append({\"role\":\"user\",\n",
        "                        \"content\": prompt})\n",
        "\n",
        "answers_list = []\n",
        "\n",
        "# With the role as 'assistant',  load the results from Astra with Vector Search.  That helps the model to provide answer to the question asked by user.\n",
        "# embedding for prompt\n",
        "print(\"Prompt : \" , prompt)\n",
        "\n",
        "embedding_query = embedding_function.embed_query(prompt)\n",
        "\n",
        "cqlSelect = f'SELECT document FROM {ASTRA_DB_KEYSPACE}.{ASTRA_DB_TABLE_NAME} ORDER BY embedding_vector ANN OF {embedding_query} LIMIT 5;'\n",
        "rows = session.execute(cqlSelect)\n",
        "for row_i, row in enumerate(rows):\n",
        "    brand_dict = {'role': \"assistant\", \"content\": f\"{row.document}\"}\n",
        "    answers_list.append(brand_dict)\n",
        "\n",
        "message_objects.extend(answers_list)\n",
        "message_objects.append({\"role\": \"assistant\", \"content\": \"Here's my answer to your question.\"})\n",
        "\n",
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo-16k\",\n",
        "  messages=message_objects\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message['content'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k7DFmpRlUpX",
        "outputId": "2fa26a6c-edee-4160-ebc3-906c60ed0e71"
      },
      "id": "6k7DFmpRlUpX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt :  What is vector search?\n",
            "Vector search is a method to find related objects that have similar attributes or characteristics. It uses embeddings, which are mathematical representations of data that capture the meaning of the objects. By converting objects into vectors and using algorithms like Approximate Nearest Neighbor (ANN) search, vector search can efficiently and quickly find similar data without needing exact keywords or descriptions. Vector search is commonly used in applications like semantic search, recommendation systems, and generative AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87dgg1pvzT1F"
      },
      "source": [
        "# Content in Vector Search:  \n",
        "E.g. query the Vector Store to see what has been added to it and what happended with our documentation"
      ],
      "id": "87dgg1pvzT1F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pepS7NSo9KRO",
        "outputId": "50d5ef19-96d4-4069-d2c1-f2a898319571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Row 0:\n",
            "    document_id:      896b223d5e784110854890aeae293cf8\n",
            "    embedding_vector: [-0.009008970111608505, -0.02013927698135376, 0.0010774513939395 ...\n",
            "    document:         In Astra DB, there can be multiple Kubernetes clusters,\n",
            "each ser ...\n",
            "    metadata_blob:    {\"source\": \"/content/Whitepaper_AstraDB-Designing-Serverless-Cloud-Native-DBaaS_6141_07.22.21.pdf\", \"page\": 10}\n",
            "\n",
            "Row 1:\n",
            "    document_id:      5df6a10dac6342c7b0c344868d84761f\n",
            "    embedding_vector: [-0.005672066938132048, 0.007558133453130722, 0.0276391934603452 ...\n",
            "    document:         In\n",
            "this\n",
            "example,\n",
            "the\n",
            "vectors\n",
            "have\n",
            "two\n",
            "dimensions,\n",
            "and\n",
            "the\n",
            "entrie ...\n",
            "    metadata_blob:    {\"source\": \"/content/Vector Search for Generative AI Apps.pdf\", \"page\": 5}\n",
            "\n",
            "Row 2:\n",
            "    document_id:      0a7237da5e394061b724d61bf229d644\n",
            "    embedding_vector: [0.012285375036299229, 0.000801105925347656, -0.0165159143507480 ...\n",
            "    document:         The high-level architecture of Astra DB is shown in\n",
            "Figure 3.1.  ...\n",
            "    metadata_blob:    {\"source\": \"/content/Whitepaper_AstraDB-Designing-Serverless-Cloud-Native-DBaaS_6141_07.22.21.pdf\", \"page\": 6}\n",
            "\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "cqlSelect = f'SELECT * FROM {ASTRA_DB_KEYSPACE}.{ASTRA_DB_TABLE_NAME} LIMIT 3;'  # (Not a production-optimized query ...)\n",
        "rows = session.execute(cqlSelect)\n",
        "for row_i, row in enumerate(rows):\n",
        "    print(f'\\nRow {row_i}:')\n",
        "    print(f'    document_id:      {row.document_id}')\n",
        "    print(f'    embedding_vector: {str(row.embedding_vector)[:64]} ...')\n",
        "    print(f'    document:         {row.document[:64]} ...')\n",
        "    print(f'    metadata_blob:    {row.metadata_blob}')\n",
        "\n",
        "print('\\n...')"
      ],
      "id": "pepS7NSo9KRO"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}